= ROSA FSx Workshop
:numbered:

This lab guide provides a walkthrough on using FSx for ONTAP as a persistent storage layer for applications on *Red Hat OpenShift Service on AWS* (ROSA). It will guide you through the steps to install the NetApp Trident Container Storage Interface (CSI) driver on a ROSA cluster. Additionally, you will learn how to provision an FSx for ONTAP file system, deploy a sample stateful application. To ensure your data’s resilience, you learn how to backup and restore your application data. 


Red Hat associates that have access to the Red Hat Demo Platform (RHDP) can launch the ROSA FSx Workshop for a lab environment.

Note: This lab guide assumes you are using the RHDP environment.   


Once you have the lab environment ready,  SSH into the bastion and follow the steps below.

Steps: 

. <<lab-setup, Lab Setup>>
. <<provision-fsx-for-ontap, Provision FSx for ONTAP>>
. <<install-and-configure-the-trident-csi-driver-for-rosa, Install and Configure the Trident CSI driver for ROSA>>
. << Install and Configure the Trident CSI backend to FSx for ONTAP
. <<deploy-sample-mysql-stateful-application, Deploy a Sample MySQL Stateful Application>>
. <<backup-and-restore-with-snapshots, Backup and Restore with Snapshots>>



[[lab-setup]]
== Lab Setup

=== Login oc command to the cluster

The bastion provided with the RHDP workshop has the `aws` and `rosa` command line tools already installed and logged in.  The `oc` command is installed, but not logged in.



To login `oc` use the API URL, username, and password provided by the RHDP lab deployment.

Replace the password with your own. Remember this is senstive data, so do not share with anyone.

[source,bash]
----
oc login https://api.rosa-abc12.ab12.p1.openshiftapps.com:6443 -u cluster-admin -p {{ password }}
----

=== Clone git

Clone the git repository:

[source,bash]
----
git clone https://github.com/redhat-gpst/rosa-fsx-lab-guide.git rosa-fsx
----

`cd` into the rosa-fsx/fsx directory. This will be the main working directory for the lab.

[source,shell]
----
cd ~/rosa-fsx/fsx
----


== Provision FSx for ONTAP  [[provision-fsx]]


* FSx for NetApp ONTAP provides fully managed shared storage with features like snapshots, cloning, and data tiering. It's integrated with AWS, allowing for seamless cloud storage solutions.
* In this step, you're setting up the FSx for ONTAP file system which will be used as the backend storage for your OpenShift applications.
* We will create a single-AZ FSx for ONTAP file system in the same VPC as the ROSA cluster.

=== Get Subnet and VPC IDs

Let's take a look at the VPC subnets.

From the bastion, run the following command.  

[source,bash]
----
aws ec2 describe-subnets --query 'Subnets[*].{SubnetId:SubnetId,VpcId:VpcId,CidrBlock:CidrBlock}' \
   --output table
----

Output from commands will be showin in a box like below.

[listing]
----
-------------------------------------------------------------------------
|                             DescribeSubnets                           |
+----------------+----------------------------+-------------------------+
|    CidrBlock   |         SubnetId           |          VpcId          |
+----------------+----------------------------+-------------------------+
|  10.0.0.0/17   |  subnet-0c1e3b083f692a17f  |  vpc-0994809fd6f55252b  |
|  192.168.0.0/24|  subnet-01b8fa59d97657eca  |  vpc-0a4106cf5b3b895b5  |
|  10.0.128.0/17 |  subnet-0299fe13ba470aa9f  |  vpc-0994809fd6f55252b  |
+----------------+----------------------------+-------------------------+
----

Run the following command to assign the SubnetId to the SUBNETID variable.

[source,bash]
----
export SUBNETID=$(aws ec2 describe-subnets --query 'Subnets[?CidrBlock==`10.0.0.0/17`].SubnetId' \
   --output json | jq -r '.[0]') && echo $SUBNETID
----

Run the following command to assign the VpcId to the VPCID variable.

[source,bash]
----
export VPCID=$(aws ec2 describe-subnets --query 'Subnets[?CidrBlock==`10.0.0.0/17`].VpcId' \
   --output json | jq -r '.[0]') && echo $VPCID
----

=== Create the FSx stack

Run the following command to create the stack

[source,shell]
----
aws cloudformation create-stack \
  --stack-name ROSA-FSXONTAP \
  --template-body file://./FSxONTAP.yml \
  --region us-east-2 \
  --parameters \   
     ParameterKey=Subnet1ID,ParameterValue=$SUBNETID \
     ParameterKey=myVpc,ParameterValue=$VPCID \
     ParameterKey=FileSystemName,ParameterValue=ROSA-myFSxONTAP \
     ParameterKey=ThroughputCapacity,ParameterValue=512 \
     ParameterKey=FSxAllowedCIDR,ParameterValue=10.0.0.0/16 \
     ParameterKey=FsxAdminPassword,ParameterValue=Rosa12345 \
     ParameterKey=SvmAdminPassword,ParameterValue=Rosa12345 \
  --capabilities CAPABILITY_NAMED_IAM
----

NOTE: This can take 20 - 30 minutes

You can monitor the progress with the following command. You may have to run it a few times before the stack is fully configured.

[source,bash]
----
aws cloudformation describe-stacks --stack ROSA-FSXONTAP | grep 'StackStatus'
----
[listing]
----
  "StackStatus": "CREATE_IN_PROGRESS",
----

Once the stack is ready, you will se the `CREATE_COMPLETE` status
[listing]
----
"StackStatus": "CREATE_COMPLETE",
----

Verify your file system and storage virtual machine (SVM1) have been created.

[source,bash]
----
aws fsx describe-file-systems
----

[source,bash]
----
aws fsx describe-storage-virtual-machines
----

== Install and Configure the Trident CSI driver for ROSA [[config-trident]]

* Trident is NetApp's dynamic storage orchestrator for OpenShift. It automates and manages storage resources for containers.
* By installing Trident, you're enabling your ROSA cluster to dynamically provision and manage storage resources on FSx for ONTAP, providing a robust and scalable storage solution for your applications.

=== Install Trident

To begin, add the Astra Trident Helm repository

[source,bash]
----
helm repo add netapp-trident https://netapp.github.io/trident-helm-chart
----

Use `helm install` to install the Trident driver in the `trident` namespace. You may see a warning about Pod Security. It can be ignored.

[source,bash]
----
helm install trident netapp-trident/trident-operator --version 23.01.1 --create-namespace --namespace trident
----

Run the following command to verify the Trident driver installation.

[source,shell]
----
helm status trident -n trident | grep "NAME:" -A 5
----
[listing]
----
NAME: trident
LAST DEPLOYED: Mon Nov  6 20:52:31 2023
NAMESPACE: trident
STATUS: deployed
REVISION: 1
TEST SUITE: None
----


=== Create a secret to store the SVM username and password in the ROSA cluster

View the `svm_secret.yml` file. Take note of the password.
[source,bash]
----
cat svm_secret.yml
----
[listing]
----
apiVersion: v1
kind: Secret
metadata:
  name: backend-fsx-ontap-nas-secret
  namespace: trident
type: Opaque
stringData:
  username: vsadmin
  password: Rosa12345
----

Add the secret to the ROSA cluster with the following command:

[source, bash]
----
oc apply -f svm_secret.yml
----

To verify the secret has been added to the ROSA cluster, run the following command.

[source,bash]
----
oc get secrets -n trident | awk '/NAME|backend-fsx-ontap-nas-secret/'
----
[listing]
----
NAME                                 TYPE                                  DATA   AGE
backend-fsx-ontap-nas-secret         Opaque                                2      24h
----


== Install and Configure the Trident CSI backend to FSx for ONTAP

* The Trident backend configuration tells Trident how to communicate with the storage system, in this case, FSx for ONTAP. 
* We willl use the `ontap-nas` driver to provision storage volumes.
* We are going to edit `backend-ontap-nas.yml` so it has the IP from the ManagementLIF and DataLIF IP addresses of the FSx Server Virtual Mancine.


=== Assign the Management IP and SVM IP variables.

[source,bash]
----
export SVMIP=$(aws fsx describe-storage-virtual-machines | jq -r '.StorageVirtualMachines[].Endpoints.Management.IpAddresses[]') && echo $SVMIP
----

And then update `backend-ontap-nas.yml`

[source,bash]
----
sed -i "s/<<management-ip>>/$SVMIP/g" backend-ontap-nas.yml
----

Review the contents of the file:

[source,bash]
----
cat backend-ontap-nas.yml
----

Example:
[listing]
----
apiVersion: trident.netapp.io/v1
kind: TridentBackendConfig
metadata:
  name: backend-fsx-ontap-nas
  namespace: trident
spec:
  version: 1
  backendName: fsx-ontap
  storageDriverName: ontap-nas
  managementLIF: 10.0.50.139
  dataLIF: 10.0.50.139
  svm: SVM1
  credentials:
    name: backend-fsx-ontap-nas-secret
----

Next execute the following commands to configure the Trident backend in the ROSA cluster.

[source,shell]
----
oc apply -f backend-ontap-nas.yml
----

Verify the backend configuration.

[source,shell]
----
oc get tbc -n trident
----
[listing]
----
NAME                    BACKEND NAME   BACKEND UUID                           PHASE   STATUS
backend-fsx-ontap-nas   fsx-ontap      1f490bf3-492c-4ef7-899e-9e7d8711c82f   Bound   Success
----

== Create storage class in ROSA cluster

* A storage class defines how storage is dynamically provisioned, specifying attributes like size and performance.
* A storage class automates the creation of storage volumes when applications request storage through PVCs.
* This configures a storage class to work with Trident, ensuring efficient management of FSx for NetApp ONTAP as backend storage.

=== Create the new `trident-csi` storage class.  

Note: Use `cat` to view any files before applying them.

[source,shell]
----
oc apply -f storage-class-csi-nas.yml
----

Verify the status of the trident-csi storage class creation.

[source,shell]
----
oc get sc | awk '/NAME|trident-csi/'
----
[listing]
----
NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
trident-csi     csi.trident.netapp.io   Retain          Immediate              true                   1h58m
----

== Deploy MySQL stateful application [[deploy-mysql]]

* Stateful applications, like databases, need to maintain data across pod restarts. 
* Using PVCs, Trident ensures data persistence for these applications.
* Deploying MySQL as a sample stateful application gives a practical example of how Trident integrates with OpenShift to manage data for stateful applications, ensuring data is not lost when pods are moved or restarted.

=== Setup the MySQL Project

Before we create the MySQL application, we will creat a `mysql` project and store the application’s username and password in a Secret. 

Create the mysql namespace
[source,bash]
----
oc create ns mysql
----

We'll use the `mysql` project as our default project
[source,bash]
----
oc project mysql
----

Create the mysql secret

Note: `password` is the password but can be chagned in the `mysql-secret.yml` file

[source,bash]
----
oc apply -f mysql-secret.yml
----

Now, verify the secrets were created.

[source,bash]
----
oc get secrets | awk '/NAME|mysql-password/'
----
[listing]
----
NAME                       TYPE                                  DATA   AGE
mysql-password             opaque                                1      1h34m
----

=== Create a pvc for the MySQL application

[source,bash]
----
oc apply -f mysql-pvc.yml
----


Verify the PVCs are created by the MySQL application. 

[source,shell]
----
oc get pvc
----
[listing]
----
NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mysql-volume  Bound    pvc-676d059c-1480-4e36-963e-2249efc70331   10Gi       RWX            trident-csi    4h4m
----

=== MySQL Application Deployment

Next we will deploy the MySQL application on the ROSA cluster. 

Open `mysql-deployment.yml` and review the details –metadata, replicas, and storageclass name.
For simplicity in this lab, we are only going to create one (1) replica set.


Execute the following command.  

NOTE: Ignore any warnings about PodSecurity

[source,shell]
----
oc apply -f mysql-deployment.yml
----

Verify the application deployment.  It will take a minute for the container to start.

[source,shell]
----
oc get pods
----

[listing]
----
NAME                        READY   STATUS    RESTARTS   AGE
mysql-fsx-7db4f45b8-mmfzv   1/1     Running   0          40s

----

=== Create a service for the MySQL application

* A service in OpenShift acts as an internal load balancer. It provides a stable endpoint through which other pods within the cluster can access the MySQL application, regardless of the individual states of the MySQL pods.
* By specifying a service for MySQL, you provide a consistent internal address for the database, ensuring seamless communication even as pods are scaled or restarted.

[source,shell]
----
oc apply -f mysql-service.yml
----

Verify the service.

[source,shell]
----
oc get svc
----
[listing]
----
NAME    TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
mysql   ClusterIP   None         <none>        3306/TCP   4h3m
----

=== Create MySQL client for MySQL

* The MySQL client is used to access the MySQL application using the service we created.
* This provides a consistent entry point into the database.

Review the content of `mysql-client.yml` and then deploy the MySQL client using the following command.

[source,shell]
----
oc apply -f mysql-client.yml
----

Verify the pod status.

[source,shell]
----
oc get pods
---- 

=== Create a sample database

Log in to the MySQL client pod.

[source,shell]
----
oc exec --stdin --tty mysql-client -- sh
----

Then, Install the MySQL client tool.

[source,shell]
----
apk add mysql-client
----

Within the `mysql-client` pod, connect to the MySQL server.

[source,shell]
----
mysql -u root -p -h mysql-set-0.mysql.mysql.svc.cluster.local
----

Enter the password that is stored in mysql-secrets.yml. Once connected, Create a database on the MySQL database.

From the `MySQL [(none)]>` prompt enter the following:

[source]
----
CREATE DATABASE erp;
CREATE TABLE erp.Persons ( ID int, FirstName varchar(255),Lastname varchar(255)); 
INSERT INTO erp.Persons (ID, FirstName, LastName) values (1234 , "John" , "Doe");
commit;
select * from erp.Persons;
----

[listing]
----
+------+-----------+----------+
| ID | FirstName | Lastname |
+------+-----------+----------+
| 1234 | John | Doe |
+------+-----------+----------+
----

Type `exit` to exit the mysql server and `exit` again to exit the pod.  You should now be back at the bastion prompt

== Creating Snapshots [[creating-snapshots]]

* Snapshots are point-in-time copies of your data, crucial for backup and disaster recovery.
* Here, you’ll learn how to use Trident with FSx for ONTAP to create snapshots for backup, and how to restore your application data from these snapshots. 
* This is vital for protecting your application against data loss.


=== Create the volume snapshot class and snapshot

[source,bash]
----
oc apply -f volume-snapshot-class.yml
----

Next, create a snapshot of the exising MySQl data

[source,bash]
----
oc apply -f volume-snapshot.yml
----

Use the following to find the name of the snapshot.

[source, bash]
----
oc get volumesnapshots
----

== Data Recovery

* This part of the lab illustrates the use of snapshots in real-world scenarios through the deletion and restoration of the database.
* This demonstrates the quick and efficient data recovery capabilities of Trident and FSx for ONTAP in managing and protecting OpenShift stateful application data. 

=== Delete the `erp` database

To delete the database "erp" after creating a snapshot follow these steps:

[source,bash]
----
oc exec --stdin --tty mysql-client -n mysql -- sh
----

Login to the MYSQL database.

[source,bash]
----
mysql -u root -p -h mysql.mysql.svc.cluster.local
----




Delete the "erp" database at the `MySQL [(none)]>` prompt

[source,sql]
----
DROP DATABASE erp;
----

After executing the DROP command, the database "erp" will be deleted, and you should see a message like:

[listing]
----
Query OK, 1 row affected
----

=== Restore the snapshot

First, create a new pvc from the snapshot.  

Note the name of the new pvc `mysql-volume-clone`

[source,bash]
----
oc apply -f mysql-pvc-clone.yml
----

=== Update the MySQL application

We need to to update the `mysql` application to point to the new pvc.

Edit `mysql-deployment.yml` with your favorite editor, `vim`

Update the last line with the name of the pvc we just created, `mysql-volume-clone`

[source]
----
  claimName: mysql-volume-clone 
----

Redeploy the application.  This will recreate the pod so it points to the cloned pvc.
[source,bash]
----
oc apply -f mysql-deployment.yml
----

Verify the new pod is running.  This may take a minute.

[source,bash]
----
oc get pods -n mysql
----

=== Validate the Database Restored

* Validation confirms that the restored data is complete and accurate, maintaining the integrity of the database after a recovery process.
* Validation helps in identifying any issues or gaps in the restoration process, allowing for timely corrections


We can now check that our data has been restored.


[source,bash]
----
oc exec --stdin --tty mysql-client -n mysql -- sh
----
[source,bash]
----
mysql -u root -p -h mysql.mysql.svc.cluster.local
----

Show Databases

[source,sql]
----
MySQL [(none)]> SHOW DATABASES;
+--------------------+
| Database           |
+--------------------+
| erp                |
+--------------------+
----

Show database data

[source,sql]
----
MySQL [(none)]> select * from erp.Persons;
+------+-----------+----------+
| ID   | FirstName | Lastname |
+------+-----------+----------+
| 1234 | John      | Doe      |
+------+-----------+----------+
----


Congrats.  You have completed the lab!